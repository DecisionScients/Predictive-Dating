Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{scikit-learn,
author = {Pedregosa, F and Varoquaux, G and Gramfort, A and Michel, V and Thirion, B and Grisel, O and Blondel, M and Prettenhofer, P and Weiss, R and Dubourg, V and Vanderplas, J and Passos, A and Cournapeau, D and Brucher, M and Perrot, M and Duchesnay, E},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in {\{}P{\}}ython}},
volume = {12},
year = {2011}
}
@book{Data2007,
abstract = {This is an introductory course in machine learning (ML) that covers the basic theory, algorithms, and applications. ML is a key technology in Big Data, and in many financial, medical, commercial, and scientific applications. It enables computational systems to adaptively improve their performance with experience accumulated from the observed data. ML has become one of the hottest fields of study today, taken up by undergraduate and graduate students from 15 different majors at Caltech. This course balances theory and practice, and covers the mathematical as well as the heuristic aspects.},
author = {Data, Learning From},
isbn = {9780471681823},
keywords = {rning from data},
title = {{LEARNING FROM DATA}},
url = {https://work.caltech.edu/telecourse},
year = {2007}
}
@article{JamesHastie2013,
abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a {\鏖翳秕忮轭轭糸黹溽糸铉ㄔ桐ㄒ轫狍物蝣衢玲歪翳屙狒殂犰义鲩鬻蟋审篚舶胺姗序镤蹉腻筱蜷痿轱澡轶轶珧徜踽翦戾鲥翦翕镲镱礤狍躜翳屣蝙犷痱镡徕殪轸翳屣蝙澡怙镫汜忮躞邃狍翦骘赭箦礤篝弪箦聃孱沐镦泔躜箦轭礤狍躜翳屣蝙犷痱镡徕殪轸翳屣蝙鏖翳犷镳糸镱麸轭沆蹁篚痧戾礤铘犰磲翦蜷犰镱篝镢栳篝殂痱镢弩箦犷箴邈獒麸痖泱婶轶轭翦钿邃痱轫狎殪骘骈蝮遽需漠篝蹁孱趔轭磲翳屙狒殂犷篝狒轶糸泱犰翳秕玷磲翳屙狒殂犰禊徜鲠钽邃篝蹁孱趔骝镯孱玳铄弪轭犷邈镱镯殂黠蹯犰箫骈钿翳怙镫躞彐蹯序弪羼蹰箝翦狎脲痿麸翳黹铋磲戾鲥镦犷躅溴蝮翎钿轭镦忉箝蝈犰犷犰箝泔钽屦趔篚汨狍扉黹趔泔铘轭蹰豉溟骀弪孱糸徕殪轸议屙犷轭翦珧狒轱瞵犷泔铞弪珏钽镦箦聃孱沐犷箦蜷弩蝈鲩鬻镦翳轶磲翦蜷犰轶轭沆蹁邃轭翳狃疱钿轼澡怙镫篝狎趔鏖翳犷轭骘蝽犰轭趄镤蹉糸镱翳狒痱秭殇弩箫礤桢躜轶糸泱轭麸翳徕篝蜥泗泔钽屦趔镦礤狍躜犷轭翦珧狒轱翳屣蝙麒殂狎翳孱蜷顼蝻躞禊溴鲥祜疱洚澡骈蝮疳螋镦翳怙镫汜忮躞邃骘篝犷溽蜾蝈犰犷犰箝泔躜箦骘怙翳磲翳屙狒殂犷篝狒轶糸泱需漠篝蹁孱趔狍轸痱秭殇弩骢祆泔鲥蜥珏镦麸痖泱篚汨狍翳泔铙趄蹉糸镱镦体忮箸蹂郁殄祠赍礤狍躜弩镱蝈犰扉铄犷捧沆殇遽箴徙弩翳忉箝泔铞弪珏钽翳屣蝈眢箴徙弩箝珙邃礤狍躜弩裔滹瞽伍腼澌翳屣蝈憩体忮箸蹂溴泔眇矬轸轱翳屣蝈犷翳骢钿犴孱翎翳屣蝈镦体忮箸蹂轭翦珧狒轱镱椰痱镤蹉箴徙弩犷痱镤蹉礤狍躜弩犷契忾铋燥铄祆翳屣蝈眢婶犰箫痱秭殇弩犷屐屙孱翎蝙轭趄镤蹉糸镱麸箩钺汨犷乳焘弪箴徙弩泔铞镬豸轱铙骑躜殄箦蜷弩犷骑躜殄犷徐犷汨弪屐趄犷箧矧眢澡躞疳螋黠蹯忮疳螋殂蹯狎禊躞彐蹯骘篝蹁孱趔轭豉痖汜郁狒轶糸泱需漠痱镧蜥殒箦疳蜥翦泔躜箦镱蝈犰犷犰箝轶铒篝犷溽蜾蝈聃轵屙孱舢嗅螋缮ㄣ栳痿弪董背痱秭殇弩骢祆泔鲥蜥珏镦篝犷溽蜾珧徜踽翦戾鲥痱镡徕殪轸翳屣蝙婶篝狎趔鏖翳孙祉镧矧秭痱镡徕殪轸盹溴犷孙祉镧矧秭屮轶翦钽翳屣蝈懋婶翳孱趄遽趔翳矧秕玷禊翳灬黧镦灬蜱铛礅弪轭沆蹁轭蝈铄麽翳屣蝙犷弪顼溟翳屣蝈眢鏖翳狃痨殂狒轱铙犷翳孱麇犭泔铞弪珏钽镦痱镡徕殪轸溟篝蜷怩糸镱蟋汨狎徙翦蜷篝殂骢钽糸镱蟋翳体鳄抿犴弪泔铘轭蹰豉翳屣蝈犷翳沐铘蜥扉黹翳屣蝈狍麇祆狍篝徕戾灬黧婶孱潴鏖翳泔钿轸轱钺屮疱泗狒轱铙犷泔钿轸轱钺痱镡徕殪轸犷犷轭趄镤蹉糸镱麸翳翳屣蝙镦溟筱蝈翦糸礤磲螋轭玑戾螽嗅螋缮ㄣ栳痿弪贝备痱秭殇弩盹溴篝泔鲥蜥珏镦溟筱蝈翦糸礤歪螂秭汨衢铙鏖翳泔躅翎忪犷珏铄蜥篝狒箴徙弩兔兔泔铘轭躏躞糸礤溟筱蝈翦箴徙牾眇歪螂秭痱镢弩箦蟋买秣铋犷盹糸镱黹轭箦聃孱沐蟋怙雉篝蜥礤翳镤蟋犷怛犷汨轭痱镢弩箦螽婶泔蹯忮躞邃骘麸痖泱箦黹钺泔躜箦矧狍犷轭趄镤蹉糸镱麸篝镢栳篝殂痱镢弩箦螽乞镯翳蝈鲩鬻蠛澡弪狎轭翦蝈篝轭犷铒瞽篝犷溽蜾麸痖泱翳狒狎铒躞踽祆轭沆蹁邃轭骈蝮泔躜箦轭礤狍趱蝈翳屣蝈糸痱镡徕殪轸轭沆蹁轭歪螂秭描衢铙犷兔兔翳怙雉篝蜥瓞扉黹翳屣蝈眢骘磲螋轭玑戾犷黹轭箦聃孱沐蟋买秣铋犷盹糸镱犷歪螂秭痱镢弩箦螽澡磲翦蜷犰轶麇祆篚痫螋邃鏖翳磲铢孱洵镦汨狃翦痱镡戾眢漠坍豌体轶骘予矧嘛镫义鲩鬻镦翳捎涩腻沐礅弪舶岸狎汨轹逍蝈骈狎亻鳊狎轹射狎亻龊卑北倍豆龀狨翳矧轻蝈翳梳礤犷尼铋屐岈组趑孱犷则弼矧柔篝殄犷蚤怏栝蜥铋绎忮螋滹卑卑倍戤疱鲠舶胺岸鞍洱屦蜷铘狎亻龊卑北倍豆龀轶忸狗赴掣贩副父待轶箢氨恫贝倒痦殇卑贡卑倍糸綮令深趄镤蹉糸镱麸郁狒轶糸汜体狎铋铉躜梏麴蠛鼢鳝忏娈躞惝邃醑gareth/ISL/ http://books.google.com/books?id=9tv0taI8l6YC},
volume = {8},
year = {2013}
}
@book{strang2006linear,
address = {Belmont, CA},
author = {Strang, Gilbert},
isbn = {0030105676 9780030105678 0534422004 9780534422004},
keywords = {book math to{\_}READ},
publisher = {Thomson, Brooks/Cole},
title = {{Linear algebra and its applications}},
url = {http://www.amazon.com/Linear-Algebra-Its-Applications-Edition/dp/0030105676},
year = {2006}
}
@misc{Nga,
abstract = {Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.},
author = {Ng, Andrew},
title = {{Machine Learning | Coursera}},
url = {https://www.coursera.org/learn/machine-learning/home/info},
urldate = {2017-09-16}
}
@book{savov2017no,
abstract = {Linear algebra is the foundation of science and engineering. Knowledge of linear algebra is a prerequisite for studying statistics, machine learning, computer graphics, signal processing, chemistry, economics, quantum mechanics, and countless other applications. Indeed, linear algebra offers a powerful toolbox for modelling the real world. The NO BULLSHIT guide to LINEAR ALGEBRA shows the connections between the computational techniques of linear algebra, their geometric interpretations, and the theoretical foundations. This university-level textbook contains lessons on linear algebra written in a style that is precise and concise. Each concept is illustrated through definitions, formulas, diagrams, explanations, and examples of real-world applications. Readers build their math superpowers by solving practice problems and learning to use the computer algebra system SymPy to speed up tedious matrix arithmetic tasks.},
author = {Savov, I},
isbn = {9780992001025},
publisher = {Minireference Publishing},
title = {{No Bullshit Guide to Linear Algebra}},
url = {https://books.google.com/books?id=A4WzswEACAAJ},
year = {2017}
}
@article{Brieman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * * , 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Brieman, Leo},
file = {::},
journal = {Machine learning},
keywords = {classification,ensemble,regression},
pages = {5--32},
title = {{Random Forests}},
url = {https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat06520},
volume = {45},
year = {2014}
}
@misc{wiki:unsupervisedlearning,
annote = {[Online; accessed 
8-September-2017
]},
author = {Wikipedia},
title = {{Unsupervised learning --- Wikipedia{\{},{\}} The Free Encyclopedia}},
url = {https://en.wikipedia.org/w/index.php?title=Unsupervised{\_}learning{\&}oldid=793838440},
year = {2017}
}
@article{Samuel,
abstract = {Two machine-learning procedures have been investigated in some detail usi!Jg the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to playa better game of checkers than can be played by the person who wrote the program. Further-more, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these 'experiments are, of course, applicable to many other situations.},
author = {Samuel, Arthur L},
file = {::},
title = {{4.3.3 Some Studies in Machine Learning Using the Game of Checkers Some Studies in Machine Learning Using the Game of Checkers}},
url = {https://www.cs.virginia.edu/{}evans/greatworks/samuel1959.pdf}
}
@book{Mitchell1997,
abstract = {Mitchell covers the field of machine learning, the study of algorithms that allow computer programs to automatically improve through experience and that automatically infer general laws from specific data. 1. Introduction -- 2. Concept Learning and the General-to-Specific Ordering -- 3. Decision Tree Learning -- 4. Artificial Neural Networks -- 5. Evaluating Hypotheses -- 6. Bayesian Learning -- 7. Computational Learning Theory -- 8. Instance-Based Learning -- 9. Genetic Algorithms -- 10. Learning Sets of Rules -- 11. Analytical Learning -- 12. Combining Inductive and Analytical Learning -- 13. Reinforcement Learning.},
author = {Mitchell, Tom M. (Tom Michael)},
isbn = {0070428077},
pages = {414},
publisher = {McGraw-Hill},
title = {{Machine Learning}},
url = {http://www.cs.cmu.edu/{}tom/mlbook.html},
year = {1997}
}
@inproceedings{Caruana,
abstract = {In this paper we perform an empirical evaluation of supervised learning on high-dimensional data. We evaluate performance on three metrics: accuracy, AUC, and squared loss and study the effect of increasing dimensionality on the performance of the learning algorithms. Our findings are consistent with previous studies for problems of relatively low dimension, but suggest that as dimensionality increases the relative performance of the learning algorithms changes. To our surprise, the method that performs consistently well across all dimensions is random forests, followed by neural nets, boosted trees, and SVMs.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Caruana, Rich and Karampatziakis, Nikos and Yessenalina, Ainur},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390169},
eprint = {9605103},
file = {::},
isbn = {9781605582054},
issn = {9781605582054},
pages = {96--103},
pmid = {17255001},
primaryClass = {cs},
title = {{An empirical evaluation of supervised learning in high dimensions}},
url = {http://yann.lecun.com/exdb/mnist/ http://portal.acm.org/citation.cfm?doid=1390156.1390169},
year = {2008}
}
@misc{wiki:supervisedlearning,
annote = {[Online; accessed 
8-September-2017
]},
author = {Wikipedia},
title = {{Supervised learning --- Wikipedia{\{},{\}} The Free Encyclopedia}},
url = {https://en.wikipedia.org/w/index.php?title=Supervised{\_}learning{\&}oldid=791408094},
year = {2017}
}
@misc{DeCock,
author = {{De Cock}, Dean},
title = {{House Prices: Advanced Regression Techniques | Kaggle}},
url = {https://www.kaggle.com/c/house-prices-advanced-regression-techniques{\#}description https://www.kaggle.com/jimthompson/house-prices-advanced-regression-techniques/ensemble-model-stacked-model-example/notebook},
urldate = {2017-09-16}
}
@article{Samuel1959,
author = {Samuel, AL},
doi = {10.1147/rd.33.0210},
file = {::},
isbn = {0018-8646},
issn = {0018-8646},
journal = {IBM Journal of research and development},
number = {3},
pages = {210--229},
title = {{Some studies in machine learning using the game of checkers}},
url = {https://pdfs.semanticscholar.org/e9e6/bb5f2a04ae30d8ecc9287f8b702eedd7b772.pdf http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5392560},
volume = {3},
year = {1959}
}
@book{hastie01statisticallearning,
abstract = {The area's standard text revised and expanded. During the past decade has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book descibes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression {\&} path algorithms for the lasso, non-negative matrix factorization and spectral clustering. There is also a chapter on methods for ``wide'' data ( p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit.},
address = {New York, NY, USA},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
publisher = {Springer New York Inc.},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning}},
year = {2001}
}
@misc{matlab,
title = {{Unsupervised Learning - MATLAB {\&} Simulink}},
url = {https://www.mathworks.com/discovery/unsupervised-learning.html},
urldate = {2017-09-08}
}
@misc{TheMathworksInc.2016,
abstract = {The MATLAB platform is optimized for solving engineering and scientific problems. The matrix-based MATLAB language is the world's most natural way to express computational mathematics.},
author = {{The Mathworks Inc.}},
booktitle = {www.mathworks.com/products/matlab},
doi = {2016-11-26},
title = {{MATLAB - MathWorks}},
url = {https://www.mathworks.com/products/matlab.html http://www.mathworks.com/products/matlab/},
urldate = {2017-09-16},
year = {2016}
}
@misc{EatonJohn2017,
author = {{Eaton, John}, W.},
doi = {10.3169/itej.65.790},
issn = {1342-6907},
title = {{Octave}},
url = {https://www.gnu.org/software/octave/ http://ci.nii.ac.jp/naid/110009669121/ http://jlc.jst.go.jp/DN/JST.JSTAGE/itej/65.790?lang=en{\&}from=CrossRef{\&}type=abstract},
year = {2017}
}
@book{strang09,
abstract = {Book Description: Gilbert Strang's textbooks have changed the entire approach to learning linear algebra -- away from abstract vector spaces to specific examples of the four fundamental subspaces: the column space and nullspace of A and A'. Introduction to Linear Algebra, Fourth Edition includes challenge problems to complement the review problems that have been highly praised in previous editions. The basic course is followed by seven applications: differential equations, engineering, graph theory, statistics, Fourier methods and the FFT, linear programming, and computer graphics. Thousands of teachers in colleges and universities and now high schools are using this book, which truly explains this crucial subject.},
address = {Wellesley, MA},
author = {Strang, Gilbert},
edition = {Fourth},
isbn = {9780980232714 0980232716 9780980232721 0980232724 9788175968110 8175968117},
keywords = {linear.algebra matrix strang textbook},
publisher = {Wellesley-Cambridge Press},
title = {{Introduction to Linear Algebra}},
year = {2009}
}
@misc{mitchell1997machine,
author = {Mitchell, Tom M and Others},
publisher = {McGraw-Hill Boston, MA:},
title = {{Machine learning. WCB}},
year = {1997}
}
@misc{Dayan1999,
abstract = {Unsupervised learning studies how systems can learn to represent particular input patterns in a way that reflects the statistical structure of the overall collection of input patterns. By contrast with SUPERVISED LEARNING or REINFORCEMENT LEARNING, there are no explicit target outputs or environmental evaluations associated with each input; rather the unsupervised learner brings to bear prior biases as to what aspects of the structure of the input should be captured in the output.},
author = {Dayan, Peter},
booktitle = {The MIT Encyclopedia of the Cognitive Sciences},
doi = {10.1016/j.visres.2007.07.023},
issn = {0042-6989},
keywords = {Animals,Cats,Learning,Learning: physiology,Models,Neurological,Neuronal Plasticity,Neuronal Plasticity: physiology,Neurons,Neurons: physiology,Orientation,Orientation: physiology,Photic Stimulation,Photic Stimulation: methods,Sensory Deprivation,Sensory Deprivation: physiology,Visual Cortex,Visual Cortex: physiology,Visual Perception,Visual Perception: physiology},
number = {22},
pages = {1--29},
pmid = {17850840},
title = {{Unsupervised Learning}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17850840},
urldate = {2017-09-08},
volume = {47},
year = {1999}
}
@misc{Leek,
abstract = {One of the most common tasks performed by data scientists and data analysts are prediction and machine learning. This course will cover the basic components of building and applying prediction functions with an emphasis on practical applications. The course will provide basic grounding in concepts such as training and tests sets, overfitting, and error rates. The course will also introduce a range of model based and algorithmic machine learning methods including regression, classification trees, Naive Bayes, and random forests. The course will cover the complete process of building prediction functions including data collection, feature creation, algorithms, and evaluation.},
author = {Leek, Jeff PhD and Peng, Roger D. PhD and {Caffo, Brian}, PhD},
title = {{Coursera | Practical Machine Learning | Data Science Specialization by Johns Hopkins University}},
url = {https://www.coursera.org/learn/practical-machine-learning},
urldate = {2017-09-08}
}
@misc{Ng,
abstract = {Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI. This course provides a broad introduction to machine learning, datamining, and statistical pattern recognition. Topics include: (i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI). The course will also draw from numerous case studies and applications, so that you'll also learn how to apply learning algorithms to building smart robots (perception, control), text understanding (web search, anti-spam), computer vision, medical informatics, audio, database mining, and other areas.},
author = {Ng, Andrew},
title = {{Machine Learning by Stanford University}},
url = {https://www.coursera.org/learn/machine-learning/home/welcome},
urldate = {2017-09-08},
year = {2015}
}
@misc{UniversityofWash,
abstract = {This Specialization from leading researchers at the University of Washington introduces you to the exciting, high-demand field of Machine Learning. Through a series of practical case studies, you will gain applied experience in major areas of Machine Learning including Prediction, Classification, Clustering, and Information Retrieval. You will learn to analyze large and complex datasets, create systems that adapt and improve over time, and build intelligent applications that can make predictions from data.},
author = {{University of Washington}},
keywords = {Coursera,certificates,courses,education,free,mooc,online,specializations},
title = {{Machine Learning - University of Washington | Coursera}},
url = {https://www.coursera.org/specializations/machine-learning https://www.coursera.org/course/machlearning},
urldate = {2017-09-08}
}
