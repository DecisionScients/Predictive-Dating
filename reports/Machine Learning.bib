Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{scikit-learn,
author = {Pedregosa, F and Varoquaux, G and Gramfort, A and Michel, V and Thirion, B and Grisel, O and Blondel, M and Prettenhofer, P and Weiss, R and Dubourg, V and Vanderplas, J and Passos, A and Cournapeau, D and Brucher, M and Perrot, M and Duchesnay, E},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in {\{}P{\}}ython}},
volume = {12},
year = {2011}
}
@book{Data2007,
abstract = {This is an introductory course in machine learning (ML) that covers the basic theory, algorithms, and applications. ML is a key technology in Big Data, and in many financial, medical, commercial, and scientific applications. It enables computational systems to adaptively improve their performance with experience accumulated from the observed data. ML has become one of the hottest fields of study today, taken up by undergraduate and graduate students from 15 different majors at Caltech. This course balances theory and practice, and covers the mathematical as well as the heuristic aspects.},
author = {Data, Learning From},
isbn = {9780471681823},
keywords = {rning from data},
title = {{LEARNING FROM DATA}},
url = {https://work.caltech.edu/telecourse},
year = {2007}
}
@article{JamesHastie2013,
abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a {\~{}}without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gareth, James and Daniela, Witten and Trevor, Hastie and Tibshirani, Robert},
doi = {10.1016/j.peva.2007.06.006},
eprint = {arXiv:1011.1669v3},
isbn = {9780387781884},
issn = {01621459},
pmid = {10911016},
title = {{An Introduction to Statistical Learning}},
url = {https://www-bcf.usc.edu/{~}gareth/ISL/ http://books.google.com/books?id=9tv0taI8l6YC},
volume = {8},
year = {2013}
}
@book{strang2006linear,
address = {Belmont, CA},
author = {Strang, Gilbert},
isbn = {0030105676 9780030105678 0534422004 9780534422004},
keywords = {book math to{\_}READ},
publisher = {Thomson, Brooks/Cole},
title = {{Linear algebra and its applications}},
url = {http://www.amazon.com/Linear-Algebra-Its-Applications-Edition/dp/0030105676},
year = {2006}
}
@misc{Nga,
abstract = {Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.},
author = {Ng, Andrew},
title = {{Machine Learning | Coursera}},
url = {https://www.coursera.org/learn/machine-learning/home/info},
urldate = {2017-09-16}
}
@book{savov2017no,
abstract = {Linear algebra is the foundation of science and engineering. Knowledge of linear algebra is a prerequisite for studying statistics, machine learning, computer graphics, signal processing, chemistry, economics, quantum mechanics, and countless other applications. Indeed, linear algebra offers a powerful toolbox for modelling the real world. The NO BULLSHIT guide to LINEAR ALGEBRA shows the connections between the computational techniques of linear algebra, their geometric interpretations, and the theoretical foundations. This university-level textbook contains lessons on linear algebra written in a style that is precise and concise. Each concept is illustrated through definitions, formulas, diagrams, explanations, and examples of real-world applications. Readers build their math superpowers by solving practice problems and learning to use the computer algebra system SymPy to speed up tedious matrix arithmetic tasks.},
author = {Savov, I},
isbn = {9780992001025},
publisher = {Minireference Publishing},
title = {{No Bullshit Guide to Linear Algebra}},
url = {https://books.google.com/books?id=A4WzswEACAAJ},
year = {2017}
}
@article{Brieman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * * , 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Brieman, Leo},
file = {::},
journal = {Machine learning},
keywords = {classification,ensemble,regression},
pages = {5--32},
title = {{Random Forests}},
url = {https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat06520},
volume = {45},
year = {2014}
}
@misc{wiki:unsupervisedlearning,
annote = {[Online; accessed 
8-September-2017
]},
author = {Wikipedia},
title = {{Unsupervised learning --- Wikipedia{\{},{\}} The Free Encyclopedia}},
url = {https://en.wikipedia.org/w/index.php?title=Unsupervised{\_}learning{\&}oldid=793838440},
year = {2017}
}
@article{Samuel,
abstract = {Two machine-learning procedures have been investigated in some detail usi!Jg the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to playa better game of checkers than can be played by the person who wrote the program. Further-more, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these 'experiments are, of course, applicable to many other situations.},
author = {Samuel, Arthur L},
file = {::},
title = {{4.3.3 Some Studies in Machine Learning Using the Game of Checkers Some Studies in Machine Learning Using the Game of Checkers}},
url = {https://www.cs.virginia.edu/{~}evans/greatworks/samuel1959.pdf}
}
@book{Mitchell1997,
abstract = {Mitchell covers the field of machine learning, the study of algorithms that allow computer programs to automatically improve through experience and that automatically infer general laws from specific data. 1. Introduction -- 2. Concept Learning and the General-to-Specific Ordering -- 3. Decision Tree Learning -- 4. Artificial Neural Networks -- 5. Evaluating Hypotheses -- 6. Bayesian Learning -- 7. Computational Learning Theory -- 8. Instance-Based Learning -- 9. Genetic Algorithms -- 10. Learning Sets of Rules -- 11. Analytical Learning -- 12. Combining Inductive and Analytical Learning -- 13. Reinforcement Learning.},
author = {Mitchell, Tom M. (Tom Michael)},
isbn = {0070428077},
pages = {414},
publisher = {McGraw-Hill},
title = {{Machine Learning}},
url = {http://www.cs.cmu.edu/{~}tom/mlbook.html},
year = {1997}
}
@inproceedings{Caruana,
abstract = {In this paper we perform an empirical evaluation of supervised learning on high-dimensional data. We evaluate performance on three metrics: accuracy, AUC, and squared loss and study the effect of increasing dimensionality on the performance of the learning algorithms. Our findings are consistent with previous studies for problems of relatively low dimension, but suggest that as dimensionality increases the relative performance of the learning algorithms changes. To our surprise, the method that performs consistently well across all dimensions is random forests, followed by neural nets, boosted trees, and SVMs.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Caruana, Rich and Karampatziakis, Nikos and Yessenalina, Ainur},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390169},
eprint = {9605103},
file = {::},
isbn = {9781605582054},
issn = {9781605582054},
pages = {96--103},
pmid = {17255001},
primaryClass = {cs},
title = {{An empirical evaluation of supervised learning in high dimensions}},
url = {http://yann.lecun.com/exdb/mnist/ http://portal.acm.org/citation.cfm?doid=1390156.1390169},
year = {2008}
}
@misc{wiki:supervisedlearning,
annote = {[Online; accessed 
8-September-2017
]},
author = {Wikipedia},
title = {{Supervised learning --- Wikipedia{\{},{\}} The Free Encyclopedia}},
url = {https://en.wikipedia.org/w/index.php?title=Supervised{\_}learning{\&}oldid=791408094},
year = {2017}
}
@misc{DeCock,
author = {{De Cock}, Dean},
title = {{House Prices: Advanced Regression Techniques | Kaggle}},
url = {https://www.kaggle.com/c/house-prices-advanced-regression-techniques{\#}description https://www.kaggle.com/jimthompson/house-prices-advanced-regression-techniques/ensemble-model-stacked-model-example/notebook},
urldate = {2017-09-16}
}
@article{Samuel1959,
author = {Samuel, AL},
doi = {10.1147/rd.33.0210},
file = {::},
isbn = {0018-8646},
issn = {0018-8646},
journal = {IBM Journal of research and development},
number = {3},
pages = {210--229},
title = {{Some studies in machine learning using the game of checkers}},
url = {https://pdfs.semanticscholar.org/e9e6/bb5f2a04ae30d8ecc9287f8b702eedd7b772.pdf http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5392560},
volume = {3},
year = {1959}
}
@book{hastie01statisticallearning,
abstract = {The area's standard text revised and expanded. During the past decade has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book descibes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression {\&} path algorithms for the lasso, non-negative matrix factorization and spectral clustering. There is also a chapter on methods for ``wide'' data ( p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit.},
address = {New York, NY, USA},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
publisher = {Springer New York Inc.},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning}},
year = {2001}
}
@misc{matlab,
title = {{Unsupervised Learning - MATLAB {\&} Simulink}},
url = {https://www.mathworks.com/discovery/unsupervised-learning.html},
urldate = {2017-09-08}
}
@misc{TheMathworksInc.2016,
abstract = {The MATLAB platform is optimized for solving engineering and scientific problems. The matrix-based MATLAB language is the world's most natural way to express computational mathematics.},
author = {{The Mathworks Inc.}},
booktitle = {www.mathworks.com/products/matlab},
doi = {2016-11-26},
title = {{MATLAB - MathWorks}},
url = {https://www.mathworks.com/products/matlab.html http://www.mathworks.com/products/matlab/},
urldate = {2017-09-16},
year = {2016}
}
@misc{EatonJohn2017,
author = {{Eaton, John}, W.},
doi = {10.3169/itej.65.790},
issn = {1342-6907},
title = {{Octave}},
url = {https://www.gnu.org/software/octave/ http://ci.nii.ac.jp/naid/110009669121/ http://jlc.jst.go.jp/DN/JST.JSTAGE/itej/65.790?lang=en{\&}from=CrossRef{\&}type=abstract},
year = {2017}
}
@book{strang09,
abstract = {Book Description: Gilbert Strang's textbooks have changed the entire approach to learning linear algebra -- away from abstract vector spaces to specific examples of the four fundamental subspaces: the column space and nullspace of A and A'. Introduction to Linear Algebra, Fourth Edition includes challenge problems to complement the review problems that have been highly praised in previous editions. The basic course is followed by seven applications: differential equations, engineering, graph theory, statistics, Fourier methods and the FFT, linear programming, and computer graphics. Thousands of teachers in colleges and universities and now high schools are using this book, which truly explains this crucial subject.},
address = {Wellesley, MA},
author = {Strang, Gilbert},
edition = {Fourth},
isbn = {9780980232714 0980232716 9780980232721 0980232724 9788175968110 8175968117},
keywords = {linear.algebra matrix strang textbook},
publisher = {Wellesley-Cambridge Press},
title = {{Introduction to Linear Algebra}},
year = {2009}
}
@misc{mitchell1997machine,
author = {Mitchell, Tom M and Others},
publisher = {McGraw-Hill Boston, MA:},
title = {{Machine learning. WCB}},
year = {1997}
}
@misc{Dayan1999,
abstract = {Unsupervised learning studies how systems can learn to represent particular input patterns in a way that reflects the statistical structure of the overall collection of input patterns. By contrast with SUPERVISED LEARNING or REINFORCEMENT LEARNING, there are no explicit target outputs or environmental evaluations associated with each input; rather the unsupervised learner brings to bear prior biases as to what aspects of the structure of the input should be captured in the output.},
author = {Dayan, Peter},
booktitle = {The MIT Encyclopedia of the Cognitive Sciences},
doi = {10.1016/j.visres.2007.07.023},
issn = {0042-6989},
keywords = {Animals,Cats,Learning,Learning: physiology,Models,Neurological,Neuronal Plasticity,Neuronal Plasticity: physiology,Neurons,Neurons: physiology,Orientation,Orientation: physiology,Photic Stimulation,Photic Stimulation: methods,Sensory Deprivation,Sensory Deprivation: physiology,Visual Cortex,Visual Cortex: physiology,Visual Perception,Visual Perception: physiology},
number = {22},
pages = {1--29},
pmid = {17850840},
title = {{Unsupervised Learning}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17850840},
urldate = {2017-09-08},
volume = {47},
year = {1999}
}
@misc{Leek,
abstract = {One of the most common tasks performed by data scientists and data analysts are prediction and machine learning. This course will cover the basic components of building and applying prediction functions with an emphasis on practical applications. The course will provide basic grounding in concepts such as training and tests sets, overfitting, and error rates. The course will also introduce a range of model based and algorithmic machine learning methods including regression, classification trees, Naive Bayes, and random forests. The course will cover the complete process of building prediction functions including data collection, feature creation, algorithms, and evaluation.},
author = {Leek, Jeff PhD and Peng, Roger D. PhD and {Caffo, Brian}, PhD},
title = {{Coursera | Practical Machine Learning | Data Science Specialization by Johns Hopkins University}},
url = {https://www.coursera.org/learn/practical-machine-learning},
urldate = {2017-09-08}
}
@misc{Ng,
abstract = {Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI. This course provides a broad introduction to machine learning, datamining, and statistical pattern recognition. Topics include: (i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI). The course will also draw from numerous case studies and applications, so that you'll also learn how to apply learning algorithms to building smart robots (perception, control), text understanding (web search, anti-spam), computer vision, medical informatics, audio, database mining, and other areas.},
author = {Ng, Andrew},
title = {{Machine Learning by Stanford University}},
url = {https://www.coursera.org/learn/machine-learning/home/welcome},
urldate = {2017-09-08},
year = {2015}
}
@misc{UniversityofWash,
abstract = {This Specialization from leading researchers at the University of Washington introduces you to the exciting, high-demand field of Machine Learning. Through a series of practical case studies, you will gain applied experience in major areas of Machine Learning including Prediction, Classification, Clustering, and Information Retrieval. You will learn to analyze large and complex datasets, create systems that adapt and improve over time, and build intelligent applications that can make predictions from data.},
author = {{University of Washington}},
keywords = {Coursera,certificates,courses,education,free,mooc,online,specializations},
title = {{Machine Learning - University of Washington | Coursera}},
url = {https://www.coursera.org/specializations/machine-learning https://www.coursera.org/course/machlearning},
urldate = {2017-09-08}
}
