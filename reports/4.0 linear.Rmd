

Classification problems occur as much, if not more often, than regression problems. Our focus will be on **binary classification**, where the goal is to predict a binary-valued target.  Examples of binary classification problems include:  

* A credit card company receives thousands of new credit card applications which include annual salary, outstanding debts, age, profession, etc... On the basis of existing customer payment history and profile data, you wish to categorize applications as good credit or bad credit.    

* An online music streaming company automatically plays music to its customers based upon prior listening habits and purchasing history. You are tasked with predicting whether a song that the customer has not heard before, should be played or recommended.     

* An online match making service has tens of thousands of member profiles containing personal characteristics, preferences, goals, and historical matching data.  On the basis of this, you must develop an algorithm to predict whether two people are a good match.   



## Linear Regression


## Statistical Learning

## The Linear Model
One of the most important formulas in statistics is the linear model. Given a vector of inputs $X^T = (X_1, X_2, ..., X_p)$, we predict the target $Y$ via :

$$\hat{Y}=\hat{\beta_0}+\displaystyle\sum_{j=1}^pX_j\hat\beta_j.$$
The $\hat\beta_0$ term is the intercept, or *bias* in machine learning. If we include the constant variable $1$ in $X$ and include $\hat\beta_0$ with the vector of coefficients $\hat\beta$, then we can write the linear model as the inner product of two vectors.
$$\hat{Y}=X^T\hat\beta,$$
where $X^T$ is the vector transpose. 

We've modeled a single output, so $\hat{Y}$ is a scalar. For $\hat{Y} \in \mathbb{R^n}$, where $n$ is the number of observations, $\hat{B}$ would be an $n \times p$ matrix of coefficients. This ($p+1$)-dimensional input-output space ($X,\hat{Y}$) is a *hyperplane*. If we include the constant in $X$, then the hyperplane is a subspace, which includes the origin.  If it does not include the constant, we refer to the hyperplane as an affine set which cuts the $Y$-axis at point ($0,\hat\beta_0$).  From here, out, we will assume the intercept is included in $\hat\beta$ [@hastie01statisticallearning]. 

### Fitting the Linear Model
There are several methods for fitting a linear model. Now, we focus our exploration on least squares, by far the most popular method. This gets a bit theoretical, so bear with me. The optimization objective is to find the coefficients $\beta$ that minimize the residual sum of squares (RSS):
$$RSS(\beta)=\displaystyle\sum_{i=1}^N(y_i - x_i^T\beta)^2.$$
If you recall from calculus, we minimize a function by differentiating with respect to (w.r.t.) its inputs, setting that equal to zero, then solving. To make the math a bit more tractible, we characterise RSS in matrix notation.
$$RSS(\beta)=(y-X\beta)^T(y-X\beta)$$
where $X$ is an $N \times p$ matrix of inputs, where each row is an input vector and $y$ is an $N$-vector of the outputs in the training set. We can now differentiate w.r.t. $\beta$, which produces the *normal* *equations*
$$X^T(y-X\beta)=0.$$

RSS($\beta$) is a quadratic function of the coefficients $\beta$ and therefore has a minimum. If $X^TX$ is *nonsingular*, then the *unique* solution is given by:
$$\hat\beta = (X^TX)^{-1}X^Ty.$$
Therefore, the fitted value at the $i$th input $x_i$, is $\hat{y_i}=\hat{y}(x_i)=x_i^T\hat\beta$ and the entire fitted surface is characterized by the $p$ parameters $\hat\beta$ [@hastie01statisticallearning].

That's a bit theoretical, let's look at an example of the linear model in a classification setting. consider the following plot.

```{python linear}
from sklearn.datasets import make_classification
X, y = make_classification(200, 2, 2, 0, weights=[.5, .5], random_state=15)
clf = LogisticRegression().fit(X[:100], y[:100])

xx, yy = np.mgrid[-5:5:.01, -5:5:.01]
grid = np.c_[xx.ravel(), yy.ravel()]
probs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)

fig, ax = plt.subplots(figsize=(8, 6))
ax.contour(xx, yy, probs, levels=[.5], cmap="Greys", vmin=0, vmax=.6)

ax.scatter(X[100:,0], X[100:, 1], c=y[100:], s=50,
           cmap="GnBu", vmin=-.2, vmax=1.2,
           edgecolor="white", linewidth=1)

ax.set(aspect="equal",
       xlim=(-5, 5), ylim=(-5, 5),
       xlabel="$X_1$", ylabel="$X_2$")
       
fig.savefig("./reports/figures/linear_scatter.png")
plt.close(fig)
```

![](../reports/figures/linear_scatter.png)
`r kfigr::figr(label = "linear", prefix = TRUE, link = TRUE, type="Figure")`: Linear classification scatterplot in two dimensions.

`r kfigr::figr(label = "linear", prefix = TRUE, link = TRUE, type="Figure")` shows simulated training data for a pair of inputs $X_1$ and $X_2$. The classes are coded as a binary variable $G$ (Blue=1, Green=0), and then fit by linear regression.  The line is the decision boundary defined by $x^T\hat\beta=0.5$. The area above the line corresponds to ${\{x:x^T\hat\beta > 0.5}\}$ and designates the part of the input space classified as BLUE. The area below the line correponds to ${\{x:x^T\hat\beta \le 0.5}\}$ and represents the region classified as GREEN$ish$. The fitted values $\hat{Y}$ are converted to a fitted class variable $G$ according to:
$$
\hat{G}=\left\{
\begin{array}{lr}
  \text{Blue if } \hat{Y} > 0.5, \\
  \text{Green if } \hat{Y} \leq 0.5. 
\end{array}
\right
$$

