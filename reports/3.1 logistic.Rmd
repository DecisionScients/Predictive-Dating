<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return '3.1.'+n}
}
}});</script>

## Logistic Regression
Typically used in binary classification problems, logistic regression or logit regression estimates the probability of a binary response based upon one or more predictor variables. Multinomial logistic regression is a generalization of binary logistic regression for response variables with greater than two levels. 

### Learning Objectives
Selecting and implementing the “right” machine learning algorithm to predict the future based upon past data is a necessary, but not sufficient, condition for success as a data scientist. Data science and the outcomes it produces may be complex and difficult to explain.  Yet, the effective data scientist must be equally facile at explaining her approach and findings to both colleagues and non-technical stakeholders. 

That said, by the end of this section, you will be able to:

1. Describe the Logistic Regression method and characterize its assumptions, advantages and disadvantages.    
2. Characterize problem sets for which logistic regression is best suited.   
3. Define the logistic regression hypothesis and cost functions for binary and multinomial classification problems.  
4. Explain techniques, such as batch gradient descent and stochastic gradient descent, used to estimate the parameters of the logistic function.  
5. Optimize a logistic regression in python using the scikit-learn package.   
6. Evaluate the performance of a classifier based upon logistic regression.  


### Why Logistic Regression?
We use the logistic regression model when we wish to model the posterior probability that $Y$ belongs to a particular group. But wait! Why do we need logistic regression when we can simply model $Y$ directly using linear regression methods?  Great question! 

#### Posterior Probabilities are Not Linear With Respect to the Predictors
To illustrate the point, consider the problem of predicting the probability of default on a credit card, based upon the balance [@James2017]. 

```{python default}
# Prepare data
df = pd.read_csv(os.path.join(directories.EXTERNAL_DATA_DIR, 'Default.csv'),
                 encoding="Latin-1", low_memory=False)
df['Probability of Default'] = np.where(df['default'] == 'No',0, 1)
df['Balance'] = df['balance']

# Set plot defaults
sns.set(style="whitegrid", font_scale=2)
sns.set_palette("GnBu_d")

# Render plots
fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2)
sns.regplot(x='Balance', y='Probability of Default', data=df, ci=None, ax=ax1)
sns.regplot(x='Balance', y='Probability of Default', data=df, ci=None,logistic=True, ax=ax2)
ax1.set_title(label='Linear Regression')
ax2.set_title(label='Logistic Regression')
fig.savefig("./reports/figures/default.png")
plt.close(fig)
```
![](../reports/figures/default.png)
`r kfigr::figr(label = "default", prefix = TRUE, link = TRUE, type="Figure")`: Probability of Default by Balance 

The left plot in `r kfigr::figr(label = "default", prefix = TRUE, link = TRUE, type="Figure")`  shows the estimated probability of default using linear regression. we see that the classes are linearly separable; however, the linear regression returns negative probabilities! On the right, we have the predicted probability of default using a logistic regression based upon the logistic, or sigmoid function.  All probabilities are between 0 and 1.

This illustrates several properties of the posterior probability distribution.   

1. Probabilities must be bounded between 0 and 1 for all real values of the predictor space.  There should be no credit card balance for which the probability of default is less than zero or greater than one.  
2. Probability must increase monotonomically for increasing values in the predictor space. That is to say that the probability of default must not decrease with increasing balances.  
3. Hence, the probability distribution is actually S-shaped, rather than linear over the predictor space.  

#### Linear Regression Implies Ordering in the Responses
Consider a supervised learning problem to predict the psychiatric disorder of a patient based upon electronic medical records and transcripts of past patient interviews. Suppose that there are three possible diagnoses: schizophrenia, major depressive disorder, and capgras delusion. We could encode these values as a quantitative response variable, $Y$, as follows:

$$
Y=\begin{cases}
1 \text{ if schizophrenia;} \\
2 \text{ if major depressive disorder;} \\
3 \text{ if capgras delusion.}
\end{cases}
$$
Using this coding, we could use least squares to fit a linear regression model to predict $Y$ on the basis of a set of predictors $X_1,X_2,\dots, X_p$. Unfortunately, this coding implies an ordering on the outcomes that may not exist. For instance, we could have selected an equally reasonable coding,
$$
Y=\begin{cases}
1 \text{ if major depressive disorder;} \\
2 \text{ if capgras delusion;} \\
3 \text{ if schizophrenia.}
\end{cases}\label{psych}
$$
This encoding would imply a completely different relationship between the predictors and response, resulting in very different predictions on test data.

#### Linear Regression Implies Equal Difference Between Responses
Linear regression also insists that the distances between the responses match the difference between the encodings. For instance, the encoding in $\ref{psych}$ implies that the difference between major depressive disorder and capgras delusion equals the difference between capgras delusion and schizophrenia. In this particular example, the differences between the categorical responses have no meaning. 

These scenarios illuminate the point. In many cases, there is no practical way to convert a categorical response with more than two levels into a quantitative response for linear regression. Hence, we seek to use methods that are truly suited for the qualitative response values we seek to model. If we are interested in modeling the probability that an observation belongs to a particular group, our model must provide a monotonically increasing probability between 0 and 1 for all possible values of the predictor space.

### What is Logistic Regression?
Let's first consider the binary classification context, where $y \in \lbrace0,1\rbrace$. Logistic or logit regression is a statistical model used to estimate the parameters of the sigmoid function.

#### Sigmoid Function  
The sigmoid function is defined as:
$$g(z)=\frac{1}{1+e^{-z}}\label{sigmoid}$$


```{python sigmoid}
# Create data
x = np.linspace(-10,10,100)

#sigmoid = lambda x: 1 / (1 + np.exp(-x))
def sigmoid(x):
    return (1 / (1 + np.exp(-x)))
    
# Set plot defaults
sns.set(style="whitegrid", font_scale=2)
sns.set_palette("GnBu_d")

# Render plots
fig, ax = plt.subplots()
sns.lineplot(x=x, y=sigmoid(x), ax=ax)
ax.set_title(label='Logistic (Sigmoid) Function')
ax.text(4, 0.8, r'$g(z)=\frac{1}{1+e^{-z}}$')
fig.savefig("./reports/figures/sigmoid.png")
plt.close(fig)

```

![](../reports/figures/sigmoid.png)
`r kfigr::figr(label = "sigmoid", prefix = TRUE, link = TRUE, type="Figure")`: Sigmoid Function

The sigmoid function graphically depicted in `r kfigr::figr(label = "sigmoid", prefix = TRUE, link = TRUE, type="Figure")` is an S-shaped curve, with several appealing characteristics for modeling the probability that an observation belongs to a particular class. 

1. The sigmoid function is a real function defined for all real input values.  
2. It is bounded horizontally, typically from 0 to 1, for $x\rightarrow \pm \infty$, thereby ensuring that all probabilities lie between 0 and 1.  
3. The sigmoid function has a non-negative differential at each point and therefore returns monotonically increasing values with increasing inputs. 

#### Hypothesis Function
For logistic regression, we define our hypothesis function $h_\theta$, as a special case application of the sigmoid function:
$$h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-(\theta^TX)}},$$
where:  

* $\theta \in \mathbb{R}^{n\times (p+1)}$ is the matrix of coefficients,  
* $x \in \mathbb{R}^{n\times (p+1)}$ is the matrix of inputs, and   
* $g(z)$ is defined as in $\ref{sigmoid}$. 

Hence, $g(\theta^Tx)$ gives us the probability that our class assignment is 1, in the case of a binary classification of $Y\in \lbrace 0,1\rbrace$.

In order to get our discrete 0 or 1 classification, we translate the output of $g(\theta^Tx)$ as follows:
$$
y = \begin{cases}
1, \space if \space h_\theta(x) \ge 0.5 \\
0, \space if \space h_\theta(x) \lt 0.5
\end{cases}
$$
Recall, the sigmoid function, $$g(z)=\frac{1}{1+e^{-z}},$$
computes an output greater than or equal to 0.5, when the input is greater than or equal zero. Concretely,  

* $z=0, e^0=1 \implies g(z)=1/2$  
* $z\rightarrow\infty, e^{-\infty}\rightarrow0\implies g(z)=1$   
* $z\rightarrow-\infty, e^{\infty}\rightarrow \infty \implies g(z)=0$   

So, if $\theta^TX \ge 0$, this means that $h_\theta(x)=g(\theta^Tx) \ge 0.5$ .  From these statements we can now say:  

* $\theta^TX \ge 0 \implies y=1$   
* $\theta^TX \lt 0 \implies y=0$   

From this, we can obtain the **decision boundary** which separates the area where $y=0$ and where $y=1$. For instance, let:
$$
\theta = 
\begin{bmatrix}
5\\-1\\0
\end{bmatrix}
$$
To obtain the decision boundary, we simply plug the values for $\theta$ into our logistic function.
$$y = 1, \space if\space 5x_0+(-1)x_1+0x_2 \ge 0,$$
where $x_0 = 1$. Solving the inequality, we have:
$$5-x_1 \ge 0$$
In this case, our decision boundary is a straight vertical line placed on the graph where $x_1=5$. Everything to the left of that denotes $y=1$, and everything to the right implies $y=0$.

### How Do We Fit Logistic Regression Models?
Given a training set, how do we learn the parameters $\theta$? We first define a **cost function**, a measure for how close our hypothesis function $h(x)$ is to the true response values. Then, we use a search function that starts with an "initial guess" for $\theta$, then repeatedly makes changes in $\theta$ in order to minimize the cost function until, *hopefully*, the algorithm converges at a parameter set $\theta$ that minimizes the cost function. 

#### Cost Function
We first define our cost function for logistic regression:  
$$J(\theta)=\frac{1}{n}\displaystyle\sum_{i=1}^nCost(h_\theta(x_i),y_i),$$
where:
$$
\begin{matrix}
Cost(h_\theta(x),y)=-log(h_\theta(x)) & if \space y=1 \\
Cost(h_\theta(x),y)=-log(1-h_\theta(x)) & if \space y=0
\end{matrix}
$$
```{python cost}
# Prepare data
x = np.linspace(0,1,100)
y0 = -np.log(1-x)
y1 = -np.log(x)

# Set plot defaults
sns.set(style="whitegrid", font_scale=2)
sns.set_palette("GnBu_d")

# Render plots
fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2)
sns.lineplot(x=x, y=y0, ax=ax1)
sns.lineplot(x=x, y=y1, ax=ax2)
ax1.set_title(label='Cost if y = 0')
ax2.set_title(label='Cost if y = 1')
fig.suptitle("Cost Function")
fig.savefig("./reports/figures/cost.png")
plt.close(fig)
```

`r kfigr::figr(label = "cost", prefix = TRUE, link = TRUE, type="Figure")` graphically depicts the cost function for both values of $y$.

![](../reports/figures/cost.png)
`r kfigr::figr(label = "cost", prefix = TRUE, link = TRUE, type="Figure")`: Logistic Regression Cost Function

If our correct answer is $y=0$, then the cost function will be 0 if our logistic function also outputs 0.  If our logistic function approaches 1, then the cost function approaches infinity. Conversely, if our correct answer is $y=1$, then the cost function will be 0 if our logistic function computes 1. If the logistic function approaches 0, then the cost function will approach infinity.  Note, that writing the cost function this way guarantees that $J(\theta)$ is convex for logistic regression.

Using the following representation, we can compress our cost function's two conditional cases into one case:
$$Cost(h_\theta(x),y)=-(\underbrace{ylog(h_\theta(x))}_{(1)})-(\underbrace{(1-y)log(1-h_\theta(x)))}_{(2)}$$
Note that when $y=1$, term (2) is zero and when $y=0$, term (1) is zero. Neat trick, yes?

We can therefore express the entire cost function as follows:
$$J(\theta)=-\frac{1}{n}\displaystyle\sum_{i=1}^n[y_ilog(h_\theta(x_i))+(1-y_i)log(1-h_\theta(x_i))]$$
We can implement a vectorized version as follows:  

* $g(z)=\frac{1}{1+e^{-z}}$  
* $h=g(X\theta)$    
* $J(\theta)=\frac{1}{n}[-y^Tlog(h)-(1-y)^Tlog(1-h)]$    

`r h <- 'Estimating ' $\theta$`
#### `r h`
Having defined our cost function, the next step is to identify a parameter set $\theta$ that minimizes the cost function. Generally speaking, there are two approaches we can pursue: the **analytic approach**, and the **iterative approach**. 

The analytic approach explicitly takes the derivative of the function with respect to $\theta$, sets it to zero and obtains the **normal equation**:
$$X^TX\theta=X^T\overrightarrow{y}.$$
Solving for $\theta$ gives us a closed form equation for the parameters $\theta$ that minimizes our cost function $J(\theta)$:
$$\theta=(X^TX)^{-1}X^T\overrightarrow{y}.$$
where:   
* $X$ is the matrix of input observations and   
* $y$ is the output vector.    

The problem with this approach is the time complexity of calculating $(X^TX)^{-1}$. Computing the inverse of an $n\times n$ matrix is  $O(n^3)$ and as $n$ increases it can take a very long time to finish. For small datasets ($n \lt 10,000$), the analytic approach might be practical; but, for most machine learning applications, the iterative approach is much faster.

The iterative approach starts with some 'initial guess' for $\theta$, then repeatedly changes $\theta$ to make $J(\theta)$ smaller, until *hopefully* it converges to a value of $\theta$ that minimizes $J(\theta)$. Next, we'll explore three widely used iterative optimization algorithms:  

* Gradient Descent,    
* Stochastic (Iterative) Gradient Descent, and
* Mini-Batch Gradient Descent

#### Gradient Descent
Gradient descent is an iterative optimization algorithm for finding the minimum of a function. To learn the parameters $\theta$ that minimize our cost function $J(\theta)$, gradient descent starts with an initial $\theta$ then makes successive steps proportional to the *negative* *gradient* of the cost function at the current $\theta$. Hence, each step is in the direction of the steepest descent.

![](../reports/figures/gradient-descent.png)

`r kfigr::figr(label = "gradient_descent", prefix = TRUE, link = TRUE, type="Figure")`: Gradient Descent

`r kfigr::figr(label = "gradient_descent", prefix = TRUE, link = TRUE, type="Figure")` illustrates gradient descent for two separate 'initial guesses' of $(\theta_1, \theta_2)$. Where you finish *could* depend upon where you start. Convergence to different local minimums based upon an initial position is a characteristic of gradient descent. 

##### How does Gradient Descent Work?
Let's define gradient descent more formally. Starting with some initial $\theta$, gradient descent repeatedly performs the following update:
$$\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta).$$
Computing the partial derivative, we have:
$$\theta_j:=\theta_j-\frac{\alpha}{n}\displaystyle\sum_{i=1}^n(h_\theta(x_i)-y_i)x_{i,j}\space \forall j,$$
where the $\alpha$ term is the **learning rate**.  The following equivalent notation will be familiar to you vector calculus experts.
$$\theta := \theta - \alpha\nabla H(\theta)$$
where $\nabla$ is the symbol for gradient. Subtracting the gradient ensures that each update step is taken in the direction of the steepest decrease in $J$. 

This update, called the **least mean squares** ($\mathbf{LMS}$) update rule, is performed simultaneously for all values of $j=0,\dots,p$, where $p$ is the number of predictors. The LMS update occurs once all training examples have been scanned. This is called a training *epoch*. The algorithm may span several epochs before convergence.  

We can vectorize the implementation as follows:
$$\theta:=\theta=\frac{\alpha}{n}X^T(g(X\theta)-\overrightarrow{y})$$
This algorithm is also called *batch* gradient descent because it looks at every observation in the entire training set on every step. 

##### Advantages and Disadvantages of Gradient Descent
Gradient descent is a popular optimization algorithm for good reason.
1. It is simple and effective for most convex, differentiable optimization problems.   
2. Computationally efficient. No need to compute inverses, factorizations, or 2nd derivatives of large matrices.  
3. Works in spaces with any number of dimensions.  
4. Error gradients and convergence are highly stable.  

On the other hand, gradient descent may converge slowly or inaccurately.
1. Convergence may take too many iterations if the curvature for a function is very different in different directions.  
2. Each gradient step requires $O(np)$ operations ($n=sample size, p=number of covariates$). With modern computing, data sets with $n=10^6$ and $k=10^2$ may be fine, but datasets on the order of $n=10^{12}$ may be prohibitively expensive to process. 
3. A sub-optimal learning rate may throttle the convergence process. If the learning rate is too small, it will take too many iterations to converge. If it is too large, it may oscillate around a local minimum and never converge to a true minimum. 
4. Since the entire dataset must be held in memory, certain machine learning problems may be too memory intensive.    
5. Different initial starting points may result in different local minimums.

That said, gradient descent is the workhorse of iterative optimization algorithms in machine learning. It's simple, relatively fast, and effective method for function minimization. Now, let's examine an extension of gradient descent that addresses some of its shortcomings.

#### Stochastic (Incremental) Gradient Descent

Stochastic gradient descent, also known as SGD, is an incremental, iterative, stochastic approximation of the gradient descent optimization method. Whereas gradient descent computes a true gradient from the entire training set on each iteration, stochastic gradient descent approximates the true gradient at each training observation. 

##### How does Stochastic Gradient Descent Work?
Stochastic gradient descent runs through the training set; however, unlike *batch* gradient descent, it performs the update to parameters $\theta$ after each observation. Consider the following algorithm:

  Loop {  
    for i=1 to n, {   
      $\qquad$$\theta := \theta - \alpha\nabla H(\theta)$   
    }   
  }   

Whereas batch gradient descent has to scan through the entire training set before taking a single step, SGD starts making progress right away. 
Though the convergence may be more 'noisy', SGD often converges to its approximation of the true minimum much faster than batch gradient descent. Is the stochastic gradient descent optimization close enough to that of batch gradient descent?  Great question. In practice it usually is. In fact, SGD is the standard implementation in many logistic regression packages such as scikit-learn.

##### Advantages and Disadvantages of Stochastic Gradient Descent
SGD has several advantages over batch gradient descent.  
1. Updating with each training example provides immediate insight into the performance of the model and its rate of convergence.  
2. The frequent updates often results in faster convergence for many machine learning problems. 
3. Noisy convergence can avoid sub-optimal local minimums.  

Yet, SGD can produce suboptimal results in some settings.   
1. Frequent updates introduce noise in the error gradient signal, which propogates into the parameters producing higher variance over the training epochs.  
2. The noisy error gradient may make it more difficult for the algorithm to find the error minimum.
3. Frequent updates are more computationally expensive resulting in longer convergence times for some problems.  
4. Requires a number of hyperparameters, such as the regularization parameter and number of iterations.
5. Avoiding saddle points is notoriously difficult when minimizing highly non-convex cost functions. Saddle points are points where one dimension slopes up and another slopes down. Since they are usually surrounded by a plateau of the same error, they are difficult to escape because the gradient is close to zero in all dimensions [@Dauphin].


##### Stochastic Gradient Descent Implementation Best Practices

Now, we examine another varient to gradient descent that encompasses the best features of gradient descent and SGD.

#### Mini-Batch Gradient Descent
Mini-Batch gradient descent seeks to optimize the efficiency of gradient descent with the effectiveness of SGD. Rather than computing a true gradient error from an entire data set, or an estimation based upon a single observation, mini-batch gradient descent computes the gradient against 'mini-batches' of the training set. Implementations may sum the gradient over the mini-batch or take the average which reduces variance of the gradient over the training epoch.

##### Advantages and Disadvantages of Mini-Batch Gradient Descent
Computing error gradients over 'mini-batches' provides several advantages.  
1. It allows for the use of vectorization code libraries, allowing each 'mini-batch' to be computed in parallel across multiple cores, servers, CPUs and GPUs. This can significantly accelerate the convergence process.   
2. Averaging the gradient over training examples reduce variance in the training epoch.  
3. More frequent updates (than gradient descent) provides for a more robust convergence, avoiding local minima.
4. More memory efficient since the entire dataset needn't be stored in memory.    

However, Mini-batch descent doesn't guarantee good convergence. Hyperparameters must be carefully chosen.
1. Choosing a suitable learning rate can be a challenge. If the learning rate is to small, convergence may be painfully slow. If the learning rate is too large, the algorithm may fluctuate around the minimum or even diverge [@Robbins1951].   
2. Learning rate schedules [@Darken1992] that reduce the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold, must be defined in advance.  Thus, the algorithm is unable to adapt to a dataset's characteristics.
3. 
  
1. The need to tune another hyperparameter, 'mini-batch size.



##### Stochastic Gradient Descent Implementation Best Practices

#### Newton-Raphson Method
##### What is the Newton-Raphson Method?
##### How does the Newton-Raphson Method Work?
##### Advantages and Disadvantages of Newton-Raphson Method
##### Newton-Raphson Implementation Best Practices

### Case Study
