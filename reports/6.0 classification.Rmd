## Classification 
Classification problems occur as much, if not more often, than regression problems.  Examples of classification problems include:   
1. Determine, based upon its content, whether an email is spam or not.    
2. On the basis of product reviews and customer preferences, predict whether a customer will buy a product.  
3. Predicting whether a speed dating participant will request a second date, based upon personal characteristics, interests, and demographics.  

Before we get too deep into classification, let's align on some terminology.

### Terminology and Notation
In classification problems, we have inputs, such as an intelligence rating, and outputs as in the decision to request a second date. In this series, we call inputs, *predictors* or *features*, and for outputs, we use the terms *response* or *dependent* variable. 

Classification responses belong to a finite set $G$ = {Request 2nd Date, Do not request 2nd Date}. Such qualitative responses are typically represented by numeric codes. When the size of $G$ is two, we  represent the response with a single binary variable containing either a 0 or 1. When there $K>2$ possible responses, we use a vector of $K$ binary *dummy* *variables*, only one of which is 'turned on' at a time.

A set of $j$ classification predictors or features will be denoted by a vector  $X$, of length $j$, where each feature is accessed by a subscript $X_j$. The group of classification responses is denoted by $G$. When referring to the *generic* aspects of a variable, we use capital letters such as $X$, $Y$, and $G$. Actual observations are written in lowercase. Hence, the $i$th observed value of $X$ is written as $x_i$, where $x$ is a scalar in the case of a single predictor, or a vector otherwise.

Matrices, by definition, are $m\times n$, two-dimensional arrays of values, typically in $\mathbf{R}$ (real numbers) with $m$ rows and $n$ columns. A matrix $A$ is typically indexed by a two-element subscript, $A_{ij}$, where $i$ is the row index and the $j$ is the column index.

$$\mathbf{A}=\left[\begin{array}
{rrr}
a_{11} & \dotsc & a_{1n} \\
\vdots & \ddots & \vdots \\
a_{m1} & \dotsm & a_{mn}
\end{array}\right]$$
A vector can be regarded as a special case of the matrix, where one of the matrix dimensions = 1.

Matrix transpose, denoted $^T$, reflects an operation in which the rows and columns of the matrix are swapped, e.g., row 1 becomes column 1,etc,and an $m \times n$ matrix becomes a $n \times m$ matrix, for example:

$$\mathbf{A}=\left[\begin{array}
{rrrrr}
2 & 3 & 5& -1 & 0\\
4 & 6 & -2& 5 & 3
\end{array}\right]
\mathbf{A^T}=\left[\begin{array}
{rr}
2 & 4\\
3 & 6\\
5 & -2\\
-1 & 5\\
0 & 3\\
\end{array}\right]
$$


So, we can now characterize the classification task as follows: given the value of an input vector $X$, make a good prediction of the output $Y$, denoted as $\hat{Y}$ The predictions $\hat{Y}$ typically lie in [0,1], and so we assign to $\hat{G}$ based upon $\hat{y} > 0.5$.  Our training data is represented as the set ($x_i, g_i$), where $i = 1,...,N$, and $N$ is the number of observations.

So far, so good!



### The Linear Classification Problem
We now develop a very simple, yet powerful prediction method: the linear model fit by least squares.  
Let:    
$X_1$ = Attractiveness rating  
$X_2$ = Sincerity rating   
$X_3$ = Intelligence rating   
$X_4$ = Funny rating  
$X_5$ = Ambition rating  
$X_6$ = Interests Correlation   

Consider the following observation randomly selected from our dataset.
```{python}
s = df.sample(1, random_state = 2)
```

```{r}
knitr::kable(py$s) %>%  
  kableExtra::kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = T, position = "center")
```

We can express our predictors as scalars in the column vector:
$$\mathbf{X} = \left[\begin{array}
{r}
X_1 = 4\\ 
X_2 = 7\\
X_3 = 8\\
X_4 = 5\\
X_5 = 4\\
X_6 = 0.52
\end{array}\right]$$
Hence, we can predict the second date decision, $Y$, via the model: 

$$\hat{Y} = \hat{\beta}_0 + \displaystyle\sum_{j=1}^p{X_j}\hat{\beta}_j$$ 
The $\hat{\beta}_0$ term is the intercept or bias term when $X_1 = X_2 = ... = X_6 = 0$. The $\hat{\beta}_j$ term is the coefficient for predictor $X_j$. If we include the constant variable 1 in $X$, e.g. $X_0 = 1$, we can include the $\hat{\beta}_0$ term in the vector of coefficients $\hat{\beta}$ and write the linear model in the form of an inner product;

$$\hat{Y} = X^T\hat{\beta}$$,

where $X^T$ is the transpose of the column vector $X$, into a row vector and $\hat{\beta}$ is the vector of coefficients, including the bias term. We recall from linear algebra that a cross product 

$$\begin{pmatrix}
X_0 & X_1 & X_2 & X_3 & X_4 & X_5 & X_6
\end{pmatrix}
\begin{pmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1 \\
\hat{\beta}_2 \\
\hat{\beta}_3 \\
\hat{\beta}_4 \\
\hat{\beta}_5 \\
\hat{\beta}_6
\end{pmatrix}
$$


$$\hat{Y} = X_0\hat{\beta_0} +X_1\hat{\beta_1} + X_2\hat{\beta_2} + X_3\hat{\beta_3} + X_4\hat{\beta_4} + X_5\hat{\beta_5} + X_6\hat{\beta_6}  $$
substituting the predictor values from above, we have:
$$\hat{Y} = 1\hat{\beta_0} +4\hat{\beta_1} + 7\hat{\beta_2} + 8\hat{\beta_3} + 5\hat{\beta_4} + 4\hat{\beta_5} + 0.52\hat{\beta_6}  $$

Here, we modeled a single observation, so $\hat{Y}$ is a scalar; however, $\hat{Y}$ is typically a $N$-vector representing responses for $N$ observations. In which case, $\beta$ would be a $p \times N$ matrix of coefficients.

The process of computing $\beta$, the $(p + 1) \times N$ (including the bias term $\beta_0$), is called 'fitting' the model to a set of training data.  So, how do we fit the data?  The most popular method, by far, is *least* *squares*. In this approach, we pick the coefficients $\beta$ to minimize the residual sum of squares, given by:
$$RSS(\beta) = \displaystyle\sum_{i=1}^N(y_i - x_i^T\beta)^2$$
where:   
$y_i$ represents the response for the $i$th observation,   
$x_i^T$ denotes the feature vector for the $i$th observation,   
$\beta$ is the $(p+1) \times N$ matrix of coefficients.

To find the unique solution $\beta$, we start by representing the problem in matrix notation. This is natural since our model is actually a system of linear equations. The regression coefficients $\beta$ we are solving for are the vector:  
$$
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
.\\
.\\
.\\
\beta_p
\end{pmatrix}
\in \mathbf{R}^{p+1}
$$
Similarly, each of the $n$ input samples in our training set is a $p+1$ row column vector, and $x_0$ is 1 for convenience. 





The unique solution to $\beta$ is most accessible in matrix notation.
$$RSS(\beta) = (y-X\beta)^T(y-X\beta)$$,

where:  
$X$ is an $N\times p$ matrix with each row an input vector, and $y$ is an $N$-vector of outputs in the training set. Mixing a bit of calculus with our linear algebra, we differentiate w.r.t. $\beta$, we get the *normal* equations:

$$X^T(y-X\beta) = 0$$
