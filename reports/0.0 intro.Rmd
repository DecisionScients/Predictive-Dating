> Prediction is very difficult, especially if it's about the future. - Niels Boh

# Introduction
Selecting and implementing the “right” machine learning algorithm to predict the future based upon past data is a necessary, but not sufficient, condition for success as a data scientist. Data science and the outcomes it produces may be complex and difficult to explain.  Yet, the effective data scientist must be equally facile at explaining her approach and findings to both colleagues and non-technical stakeholders.

In this ___ part series, we examine and contrast the most widely used machine learning classification algorithms.  Unlike regression algorithms which predict continuous outcomes, classifiers assign unseen observations to one or more classes, based upon insights, historical relationships and trends in the labeled training data.

## What will you learn?
By the end of this series, you will be able to:   
1.	Describe the most important machine learning classifiers   
2.	Relate their strengths, weaknesses and underlying assumptions   
3.	Identify the problems for which the algorithms are best suited   
4.	Implement the classifiers in Python   
5.	Evaluate the performance of the algorithms   

## How will we get there?   
  
1. Business Case     
    1.1. Overview    
    1.2. Data    
    1.3. Exploratory Data Analysis    
2. Core Concepts  
  2.1. Linear Algebra Review  
  2.2. 

3. Linear Classifiers   
    3.1. Logistic Regression   
    3.2. Linear Discriminant Analysis   
    3.3. K-Nearest Neighbors  
    3.4. Support Vector Machines   
4. Tree-Based Classifiers    
    4.1. Classification Tree   
    4.2. Bagging    
    4.3. Random Forests   
    4.4. Boosting   
5. Non-Linear Classifiers    
    5.1. Generalized Additive Models for Classification  
    5.2. Neural Networks   
7. Discussion  
8. Conclusions  


## Acknowledgements

This review borrows liberally from the following texts:  

* [Introduction to Statistical Learning](https://www-bcf.usc.edu/~gareth/ISL/)An accessible, clear, and well-organized overview of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and neural networks. Each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. An excellent first-exposure to machine learning.[@JamesHastie2013]

* [Elements of Statistical Learning]()[@hastie01statisticallearning] An advanced treatment of the important ideas in a variety of fields such as medicine, biology, finance, and marketing in a common conceptual framework. Emphasizing concepts, rather than mathematics, the book covers supervised learning (prediction),unsupervised learning, neural networks, support vector machines, classification trees and boosting. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. [@hastie01statisticallearning]

* [Introduction to Linear Algebra by Professor Gilbert Strang](https://www.amazon.com/Introduction-Linear-Algebra-Fourth-Gilbert/dp/0980232716/ref=sr_1_3?s=books&ie=UTF8&qid=1536621558&sr=1-3&keywords=introduction+to+linear+algebra%2C+fifth+edition+by+gilbert+strang). Credited for changing the entire approach by which we learn linear algebra, this text illustrates abstract concepts with concrete examples and applications. Highly recommended for a comprehensive treatment of linear algebra for engineers, mathematicians, and computer scientists.[@strang09]   

* [No Bullshit Guide to Linear Algebra](https://www.amazon.com/No-bullshit-guide-linear-algebra/dp/0992001021). This university level text by Ivan Savov is a great resource for acquiring a deeper understanding of challenging concepts. Its concise, precise and fun to read.[@savov2017no]   

